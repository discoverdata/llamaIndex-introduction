{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beb1a1c4-8d92-4a01-af75-d9edd8e86a76",
   "metadata": {},
   "source": [
    "![title](img/LlamaIndex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3208f7-7a9f-4746-9ad7-83972f5bb84e",
   "metadata": {},
   "source": [
    "LlamaIndex is a versatile and powerful data framework that empowers developers to connect custom data sources to large language models. \n",
    "It acts as crucial bridge, enabling LLMs to understand and reason over private or domain specific data, unlocking a wealth of potential for building \n",
    "LLM applications. Here is how LlamaIndex acheives this, broken down into its core functionalities.\n",
    "\n",
    "1. **Loading:** LlamIndex simplifies the process of ingenstion data from a wide variety of formats including databases, documents, API. It can load data from 160+ data sources and data formats (APIs, PDFs, Word documents, SQL etc.)\n",
    "2. **Indexing:** It then efficiently indexes the data creating structured representations that LLMs can easily understand. It intergrates with 40+ vector stores for similarity searches, document store for storing raw data, graph store for representing relationships and SQL db for structured data.\n",
    "3. **Querying:** LlamaIndex excels at retrieving relevant context from the indexed data. It simplifies orchestration of production-ready LLM workflows by efficently reteriving relevant context from the data whether you are building simple prompt chains, advanced Retrieval Augmented Generation (RAG) systems, ore even autonomous agents.\n",
    "4. **Evaluating:** Streamline the evalution of the LLM output with a comprehensive suit of modules. This evaluation capability is essential for fine-tuning your applications and ensuring they meet your performance requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6c44b-1915-4b43-a990-ca1b8940f9ee",
   "metadata": {},
   "source": [
    "## Applications:\n",
    "\n",
    "This framework is instrumental in building applications such as:\n",
    "* Intelligent chatbots that can answer questions based on your company's internal knowledge base.\n",
    "* Data-driven analysis tools that can extract insights from complex datasets.\n",
    "* Personalized content generation systems that can create tailored content based on user preferences and data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cecf5dc-6c83-4e38-9cf5-14c6d38f3984",
   "metadata": {},
   "source": [
    "### How to use LlamaIndex using local LLMs\n",
    "\n",
    "#### 1. Install the required libraries and Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9769917-8ab5-402d-b2a3-9ccc7222a92f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "!pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420c80eb-c376-4c7c-ad14-5c2a220acea6",
   "metadata": {},
   "source": [
    "We will use *BAAI/bge-base-en-v1.5* as our embedding model and *llama3.2* served through Ollama.\n",
    "\n",
    "Ollama is user-friendly tool designed to help setup LLMs on local machines. Moving beyond cloud and api calls, Ollama is great for experimenting with and integrating cutting edge models into various applications prioritizing privacy and local capabilities. \n",
    "\n",
    "**Key benefits**\n",
    "\n",
    "1. *Enhanced Privacy:* Sensitive data remains local ensuring greater control and security.\n",
    "2. *Faster inference:* Reduced latency due to elimination of netwwork communication.\n",
    "3. *Offline functionality:* Applications can continue to work even without internet connection.\n",
    "4. *Cost effectiveness:* Eliminates the recurring cost associated with cloud based LLM services.\n",
    "\n",
    "To install Ollama follow the [README](https://github.com/ollama/ollama?tab=readme-ov-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d42d5-a1bb-45c3-8ce6-e2b483315c1d",
   "metadata": {},
   "source": [
    "#### 2 Startup Ollama and choose the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65d7c9-d13b-4f29-8506-b3ed575bf835",
   "metadata": {},
   "source": [
    "##### 2.1 Define the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dd62191c-773f-4c9b-98ee-baf610a6c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3.1:latest\n"
     ]
    }
   ],
   "source": [
    "# Choose the model\n",
    "#OLLAMA_MODEL = 'llama3.2:1b' # 1B parameters\n",
    "#OLLAMA_MODEL = 'llama3.2' # 3B parameters\n",
    "#OLLAMA_MODEL = 'gemma3' # 4B parameters\n",
    "OLLAMA_MODEL = 'llama3.1:latest'\n",
    "\n",
    "# set it at the os level\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['OLLAMA_MODEL'] = OLLAMA_MODEL\n",
    "!echo $OLLAMA_MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af791d91-933d-49b4-97b0-978277aa6461",
   "metadata": {},
   "source": [
    "##### 2.2 Start Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b594513f-8ffa-4203-ba57-d6a045444f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✅ Ollama server started with PID 23468\n",
      "✅ Ollama server is up and running!\n"
     ]
    }
   ],
   "source": [
    "# Locally start Ollama in the background process\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "OLLAMA_COMMAND = \"ollama serve\"\n",
    "ollama_process = None\n",
    "\n",
    "# Use subprocess.Popen to start the process in the background\n",
    "try:\n",
    "    ollama_process = subprocess.Popen(\n",
    "        OLLAMA_COMMAND.split(),\n",
    "        stdout = subprocess.DEVNULL, \n",
    "        stderr = subprocess.DEVNULL\n",
    "    )\n",
    "    print(f\" ✅ Ollama server started with PID {ollama_process.pid}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error starting Ollama: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Check if server is up and running\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434\") # Default Ollama port\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Ollama server is up and running!\")\n",
    "    else:\n",
    "        print(f\"⚠️ Server responded with status code:{response.status_code}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"❌ Could not connect Ollama server. Check if it is running correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba65493c-551e-41cf-821a-12968e4c4c47",
   "metadata": {},
   "source": [
    "#### 3. Read the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5a012aa2-9c8b-41b0-9eb3-4761e2b52284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23468"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_process.pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a4949901-f725-4e9f-b645-70d925d7e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.agent.workflow import AgentWorkflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27874703-7282-4fdd-991a-f6e2e5dcf46a",
   "metadata": {},
   "source": [
    "##### 3.1 Call Ollama from LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5baa03ca-6ebb-4644-9491-67110ee3c6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are three bullet points summarizing who Sachin Tendulkar is and why he's known as the \"Master Blaster\":\n",
       "\n",
       "• **International Cricketer**: Sachin Ramesh Tendulkar is a legendary Indian cricketer who played for the Indian national team from 1989 to 2013. He holds numerous records in international cricket, including becoming the highest run-scorer in both Test and One-Day International (ODI) formats.\n",
       "\n",
       "• **Batting Genius**: Known for his exceptional batting skills, Tendulkar is renowned for his ability to score runs in all conditions, against all types of bowling. He was particularly effective at the top of the order, where he would often set up big partnerships and lay a solid foundation for his team's innings.\n",
       "\n",
       "• **Master Blaster Nickname**: The \"Master Blaster\" nickname was given to Tendulkar due to his incredible batting prowess, which earned him widespread recognition as one of the greatest batsmen in cricket history. He is credited with revolutionizing Indian cricket and inspiring a generation of young cricketers, cementing his place as an iconic figure in world cricket."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "llm = Ollama(model=OLLAMA_MODEL, request_timeout=360)\n",
    "\n",
    "# Run the completion\n",
    "response = llm.complete(\n",
    "    \"Who is Sachin Tendulkar and why is he called the master blaster? Summarize in 3 bullet points\"\n",
    ")\n",
    "\n",
    "# Display the response as Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e98896-27f4-40ad-9c66-1aaf937d613e",
   "metadata": {},
   "source": [
    "#### 4. Data Ingestion and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "420d46e0-2431-4e76-b59e-3ce66b50aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all documents from data directory and create an index\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    embed_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0480ee-c425-4cc5-9331-87f2bccdd5c6",
   "metadata": {},
   "source": [
    "#### 5. Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ac195de5-5564-40eb-8e72-0899b06bb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=Ollama(model=OLLAMA_MODEL, request_timeout=360.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc506d-3713-4aa0-8551-df2fc374b0cc",
   "metadata": {},
   "source": [
    "#### 6. Tool definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5e02533c-b58f-4867-8853-1ee398f4c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple calculator tool\n",
    "def multiply(a: float, b: float) -> int:\n",
    "    \"\"\"Useful for multiplying two numbers.\"\"\"\n",
    "    return int(a * b)\n",
    "\n",
    "# Define a document search function\n",
    "async def search_documents(query: str) -> str:\n",
    "    \"\"\"Search documents using RAG.\"\"\"\n",
    "    response = await query_engine.aquery(query)\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6dab6-2bbf-4648-ad4e-39ad4b6c5d4d",
   "metadata": {},
   "source": [
    "#### 7. Agent Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "af473336-ab18-4f43-b64a-3e3ca3ef9f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the tool call responses, I can provide you with a detailed answer.\n",
       "\n",
       "Natural Language Processing (NLP) is a rapidly evolving field that seeks to understand, interpret, and generate human language using computational methods. This definition comes from various sources, including academic papers and online articles related to NLP.\n",
       "\n",
       "Regarding your second question, 8 multiplied by 8 equals 64."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an agent workflow\n",
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [search_documents, multiply],\n",
    "    llm=Ollama(model=OLLAMA_MODEL, request_timeout=360.0),\n",
    "    system_prompt=(\n",
    "        \"You are a helpful assistant that can multiply two numbers \"\n",
    "        \"and search through documents to answer questions.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Main function\n",
    "async def main():\n",
    "    response = await agent.run(\n",
    "        \"What is Natural Language Processing, give me a defination from the document. Also what is 8 * 8?. Use tools. Give a detailed answer.\"\n",
    "    )\n",
    "\n",
    "    # Safely get content from agent response\n",
    "    if hasattr(response, \"response\") and hasattr(response.response, \"content\"):\n",
    "        final_text = response.response.content\n",
    "    elif hasattr(response, \"text\"):\n",
    "        final_text = response.text\n",
    "    else:\n",
    "        final_text = str(response)\n",
    "    \n",
    "    display(Markdown(final_text))\n",
    "    #print(response)\n",
    "\n",
    "# Run the agent properly\n",
    "if __name__ == \"__main__\":\n",
    "    await (main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7eded7-fc19-43e9-bf33-168214ece3e8",
   "metadata": {},
   "source": [
    "## Storing the RAG Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc68d34f-6451-499e-8aa1-86cf1484f9c3",
   "metadata": {},
   "source": [
    " Save the LlamaIndex to a folder (storage in this case) avoid reprocessing the documents everytime we run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecab22-ceae-46f6-8bc1-6f3a0e446e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storgae_context.persist(\"storage\")\n",
    "\n",
    "# later load the index\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    embed_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=Ollama(model=OLLAMA_MODEL, request_timeout=360.0)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_ai",
   "language": "python",
   "name": "agentic_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
